I"G<h2 id="主要内容">主要内容</h2>
<ul>
  <li><a href="#p1">ABSTRACT</a></li>
  <li><a href="#p2">一、INTRODUCTION</a></li>
  <li><a href="#p3">二、FUSION CAPABILITY OF GCNS: AN EXPERIMENTAL INVESTIGATION</a></li>
  <li><a href="#p4">三、AM-GCN: THE PROPOSED MODEL </a></li>
  <li><a href="#p5">四、EXPERIMENT</a></li>
</ul>

<h2 id="参考论文">参考论文：</h2>
<p><a href="https://arxiv.org/pdf/1709.04875v4.pdf">《Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting》</a></p>

<h2 id="正文">正文</h2>

<h3 id="abstract"><span id="p1">ABSTRACT</span></h3>
<h4 id="任务objective">任务(Objective)</h4>
<p>提出一个深度学习框架ASTGCN用于处理交通领域的时间序列预测问题。</p>
<h4 id="方法methods对研究的基本设计加以描述">方法(Methods)：对研究的基本设计加以描述。</h4>
<p>不使用正则卷积和递归单元，而是在图上表示问题，并建立具有完整卷积结构的模型，从而使训练速度更快，参数更少。</p>
<h4 id="结果results">结果(Results)</h4>
<p>实验结果表明，STGCN模型通过对多尺度交通网络的建模，有效地捕捉到了综合的时空相关性，并且一直优于现有的研究成果各种实际交通数据集的基线。</p>

<h3 id="一introduction"><span id="p2">一、INTRODUCTION</span></h3>
<h4 id="研究背景和重要性background-and-importance">研究背景和重要性(Background And Importance)</h4>
<p>交通条件和每个人的日常生活息息相关，所以准确的实时的交通预测是非常重要的。</p>
<h4 id="该领域的科研空白挑战description-of-knowledge-gap-or-chanllenge">该领域的科研空白/挑战(Description of knowledge gap or chanllenge)</h4>

<p>传统方法在短时预测上表现良好，但是在长时预测上效果不佳。
以往对中长期交通预测的研究大致可以分为两类：动态建模和数据驱动方法。
动态建模使用数学工具（如微分方程）和物理知识通过计算模拟来描述交通问题。
仿真过程不仅需要复杂的系统编程，而且需要消耗大量的计算能力。</p>

<h4 id="本文的研究课题topic-of-research-paper">本文的研究课题(Topic Of Research Paper)</h4>
<p>GCNs融合结点特征和拓扑结构。GCN学习到了以及从拓扑结构和结点特征中融合了哪些信息？</p>
<h4 id="核心方法论和主要发现结果highlight-the-approach-and-highlight-principal-findingsresult">核心方法论和主要发现/结果(Highlight The Approach And Highlight Principal Findings/Result)</h4>
<p>提出了一种用于半监督分类的自适应多通道图卷积网络（AM-GCN），该模型的中心思想是基于节点特征，拓扑结构及其组合同时学习节点嵌入。
其基本原里是，结点特征之间的相似性以及由拓扑结构推断出的相似性是相互补充的，可以自适应
地融合以得出用于分类任务地更深层地相关性信息。</p>

<p>1)为了充分利用特征空间中的信息，作者将从结点特征中生成的k个最近邻邻居图做为特征结构图。</p>

<p>2)通过特征图和拓扑图，在拓扑空间和特征空间上传播节点特征，从而使用两个特定的卷积模块在这两个空间中提取两个特定的嵌入。</p>

<p>3)考虑到两个空间之间的共同特征，我们设计了一个具有参数共享策略的公共卷积模块，以提取它们共享的公共嵌入。</p>

<p>4)我们进一步利用注意力机制自动学习不同嵌入的重要性权重，以自适应地融合它们。</p>

<p>贡献</p>

<p>1) 进行试验来测试GCNs在融合拓扑结构和点特征的能力，以及明确了GCNs的弱点。进一步研究了该问题的重要性，例如，
如何实际性的增强GCNs在分类任务上的融合能力。</p>

<p>2) 提出了最新的自适应多通道GCN框架，AM-GCN，该框架同时在拓扑和特征空间上运用了图卷积操作。结合注意力机制，
可以自适应地融合不同的信息。</p>

<p>3) 在一系列的基础数据集上的大量实验清楚地表明AM-GCN比最新的GCNs表现更好，并且AM-GCN从点特征和拓扑结构上提取了最相关的信息，
很好地应对分类任务的挑战。</p>

<h3 id="二fusion-capability-of-gcns-an-experimental-investigation"><span id="p3">二、FUSION CAPABILITY OF GCNS: AN EXPERIMENTAL INVESTIGATION</span></h3>

<p>在该节，作者使用了两个简单的实验来测试是否最新的GCNs能否自适应地学习到图中的点特征和拓扑结构，并且充分地在分类任务中融合它们。</p>

<p>作者分别建立了与点特征和拓扑结构相关地点标签(node label)，然后在两个cases上测试GCN地性能。
一个好的GCN融合能力应该能自适应地提取相关信息并得到一个好的结果。
但如果在和基线相比较时性能急剧下降，这将会证明GCN不能自适应地从点特征和拓扑结构中提取相关信息，即使点特征或者拓扑结构和点标签
有着高度地相关性。</p>

<h4 id="21-case-1-random-topology-and-correlated-node-features">2.1 Case 1: Random Topology and Correlated Node Features</h4>
<p>作者随机产生了一个包含900个结点地网络，其中任意两个节点建立边的概率时0.03。每一个结点有50维的特征向量。
为了产生点特征，作者随机给900个点赋值了3个label，并且对于带有相同标签的点，作者使用高斯分布来产生点特征。
高斯分布对于点的三个类别标签有相同的协方差矩阵，但是这三个不同中心彼此远离。
在该数据集中，点特征和点标签是具有高度相关性的，但点标签和拓扑结构没有高度相关性。</p>

<p>作者分别使用NLP和GCN训练该数据集，两个模型的分类精度分别为75.2%和100%。</p>

<p>结果符合预期。 由于节点特征与节点标签高度相关，因此MLP显示出出色的性能。
GCN从节点特征和拓扑结构中提取信息，但是无法自适应地融合它们以避免拓扑结构的干扰。 它无法与MLP的高性能匹敌。</p>

<h4 id="22-case-2-correlated-topology-and-random-node-features">2.2 Case 2: Correlated Topology and Random Node Features</h4>
<p>使用900个点产生另一个网络，这次每个点的50维特征是随机产生的。
对于拓扑结构，作者使用Stochastic Blockmodel(SBM)来将点分成3个群(nodes 0-299,300-599,600-899).
对于每个群，建立边的概率设置为0.03，不同群点之间建立边的概率为0.0015。在该数据集中，点标签是由群确定的，
例如，在同一个群中的点有相同的标签。</p>

<p>在实验中，分别使用DeepWalk和GCN来进行实验，其中，DeepWalk是忽略点特征的，两个模型的分类精度分别为100%和87%。</p>

<p>DeepWalk表现出色，因为它可以对网络拓扑结构进行全面建模。 GCN从节点特征和拓扑结构中提取信息，但是无法自适应地融合它们，以避免受到节点特征的干扰。 它无法与DeepWalk的高性能匹敌</p>

<h3 id="三am-gcn-the-proposed-model-"><span id="p4">三、AM-GCN: THE PROPOSED MODEL </span></h3>

<p><img src="../../../../../../img/in-post/2020.09/04/Figure 1.jpg" alt="Figure 1" />
Problem settings: 作者专注于属性图$G=(A,X)$中的半监督节点分类，其中$A\in\mathbb{R}^{n\times n}$是n个结点的对称邻接矩阵，
$X=\mathbb{R}^{n\times d}$是结点的特征矩阵，$d$是结点特征的纬度。
$A_{ij}=1$表示结点$i$和$j$之间有一条边。假设每一个点都属于$C$个类中的一个。
如图1所示，AM-GCN不仅允许节点特征在拓扑空间中传播，而且还可以在特征空间中传播，并且应该从这两个空间中提取与节点标签最相关的信息。
使用两个特定的卷积模块，$X$可以在特征图在拓扑图上传播并学习到特定嵌入$Z_F和Z_T$。
考虑到这两个空间图的信息有共同特征，作者设计了一个带有共享参数的common convolution module来学习the common embedding 
$Z_{CF}和Z_{CT}$,一致性约束$\mathscr{L}_c$用于增强 $Z_{CF}$和$Z_{CT}$的公共属性,
除此以外，差异性约束$\mathscr{L}_d$用于确保$Z_{F}$和$Z_{CT}$的独立性,$Z_T$和$Z_{CT}$同样如此。
考虑到结点标签和结点特征或者和拓扑结构是相关的，AM-GCN利用一个注意力机制来自适应地融合学习到地权重，
为了提取最相关信息$Z$用于分类任务。</p>
<h4 id="specific-convolution-module">Specific Convolution Module</h4>
<p>首先，为了捕捉特征空间中的潜在的结点结构，作者基于结点特征矩阵$X$构建了k-最近邻图$G_f=(A_f,X)$，其中$A_f$是KNN图的邻接矩阵。
作者首先计算n个结点之间的相似度矩阵$S\in\mathbb{R}^{n\times n}$。一下列举两种计算结点相似度的方法。其中，$x_i$和$x_j$
分别是结点i和j的特征矩阵。</p>

<p>1) Cosine Similarity:该方法利用两个特征向量之间角度的cos值计算两个结点的相似度：</p>

\[S_{ij}=\frac{x_i\cdot x_j}{|x_i|\cdot|x_j|}\tag{1}\]

<p>2) Heat Kernel：由$E_q$计算相似度。其中$t$是热传导方程中的时间参数,这里面设置$t=2$</p>

\[S_{ij}=e^{-{\frac{||x_i-x_j||^2}{t}}}\tag{2}\]

<p>将图$(A_f,X)$输入到特征空间中，l-th层的输出$Z_f^{(l)}$可以表示为：</p>

\[Z_f^{(l)}=ReLU(\tilde{D}_f^{- \frac{1}{2}}\tilde{A}_f\tilde{D}_f^{-\frac{1}{2}}{Z}_f^{(l-1)}{W}_f^{l})\tag{3}\]

<p>其中，$W_f^{(l)}$是GCN中的第l层权重矩阵，$Z_f^{0}=X$，$\tilde{A}_f=A_f+I_f$，且$\tilde{D}_f$是矩阵$\tilde{A}_f$
的对角矩阵。标记最后一层的输出嵌入为$Z_F$。通过这个方法，可以学习到点嵌入(node embedding),点嵌入在特征空间中可以捕捉到信息$Z_F$。</p>

<p>同样对于拓扑空间，输入图$G_t=(A_t,X_t)$，其中，$A_t=A$,$X_t=X$。同样，可以得到一个嵌入$Z_T$,该嵌入是在拓扑空间中提取的信息。
通过这种方法，可以更加灵活地利用最相关的信息。</p>
<h4 id="common-convolution-module">Common Convolution Module</h4>
<p>实际上，特征空间和拓扑空间并不是完全无关的。基本上，节点分类任务可以与特征空间或拓扑空间或两者中的信息相关联，这是事先很难知道的。
因此我们不仅要分别在这两种空间中提取结点嵌入信息,例如$Z_F、Z_T$，还要提取这两个空间的共同信息。
为了解决这个问题，我们设计了具有参数共享策略的Common-GCN，以在两个空间中共享嵌入。</p>

<p>首先，利用Common-GCN来从拓扑图$(A_t,X)$提取点嵌入$Z_{ct}^{(l)}$:</p>

\[Z_{ct}^{(l)}=ReLU(\tilde{D}_t^{-\frac{1}{2}}\tilde{A}_t\tilde{D}_t^{-\frac{1}{2}}Z_{ct}^{l-1}W_c^{(l)})\tag{4}\]

<p>其中$W_{c}^{(l)}$是Common-GCN的l-th层的权重矩阵，$Z_{ct}^{l-1}$是$(l-1)$层的嵌入，并且$Z_{ct}^{(0)}=X$。
当利用Common-GCN来从图$(A_f,X)$学习结点嵌入,为了提取共享信息，该模型共享了相同权重矩阵$W_c^{(l)}$:</p>

\[Z_{cf}^{(l)}=ReLU(\tilde{D}_f^{-\frac{1}{2}}\tilde{A}_f\tilde{D}_f^{-\frac{1}{2}}Z_{cf}^{l-1}W_c^{(l)})\tag{5}\]

<p>其中，$Z_{cf}^{(l)}$是l-th的输出，$Z_{cf}^{(0)}=X$。共享权重矩阵可以从两个空间中过滤出共享特征。 
根据不同的输入图，我们可以得到两个输出嵌入$Z_{CT}$和$Z_{CF}$，两个空间的共同嵌入$Z_C$为：</p>

\[Z_C=\frac{(Z_{CT}+Z_{CF})}{2} \tag{6}\]

<h4 id="attention-mechanism">Attention Mechanism</h4>
<p>目前已经获得矩阵$Z_T、Z_F、Z_C$，考虑到结点标签和这三个矩阵可能有不同程度的相关性，所以使用注意力机制来学习这三个矩阵的
重要性：</p>

\[(\alpha_t,\alpha_c,\alpha_f)=att(Z_T,Z_C,Z_F)\tag{7}\]

<p>这里面，$\alpha_t,\alpha_c,\alpha_f\in\mathbb{R}^{n\times 1}$分别表示以上三个矩阵对n个结点的注意力得分。</p>

<p>这里以结点i举例，该节点的在$Z_T$中的嵌入为$z_T^i\in\mathbb{R}^{1\times h}$.首先先使用非线性变换转换该嵌入，
然后使用一个共享的向量$q\in\mathbb{R}^{h’\times 1}$来获得注意力得分$w_T^i$:</p>

\[w_T^i=q^T\cdot tanh(W_T\cdot(Z_T^i)^T+b_T)\tag{8}\]

<p>其中$W_T\in\mathbb{R}^{h’\times h}$是权重矩阵，$b_T\in\mathbb{R}^{h’\times 1}$是偏移向量，同理可得
$w_C^i,w_F^i$。通过标准化$w_T^i,w_C^i,W_F^i$可得最终的权重</p>

\[\alpha_T^i=softmax(w_T^i)=\frac{exp{(w_T^i)}}{exp{(w_T^i)}+exp{(w_C^i)}+exp{(w_F^i)}}\tag{9}\]

<p>同理可得，$alpha_C^i=softmax(w_C^i)$和$\alpha_F^i=softmax(w_F^i)$。
对于所有的n个结点，可以得到$\alpha_t=[\alpha_T^i],\alpha_c=[\alpha_C^i],\alpha_f=[\alpha_F^i]\in\mathbb{R}^{n\times 1}$
,标记$\alpha_T=diag(\alpha_t),\alpha_C=diag(\alpha_c),\alpha_F=diag(\alpha_f)$。
然后结合这三个嵌入获得最终的嵌入：</p>

\[Z=\alpha_T\cdot Z_T+\alpha_C\cdot Z_C+\alpha_F\cdot Z_F\tag{10}\]

<h4 id="34-objective-function">3.4 Objective Function</h4>
<h5 id="341-consistency-constraint">3.4.1 Consistency Constraint</h5>
<p>对于两个Common-GCN的输出嵌入$Z_{CT}$和$Z_{CF}$，即使Common-GCN有共享的权重
矩阵，为了进一步增强他们的共同信息，作者仍然设计了一个一致性约束。</p>

<p>首先，使用$L_2-normalization$来标准化嵌入矩阵为$Z_{CTnor}$和$Z_{CFnor}$。
然后，两个标准矩阵可以用来捕捉n个结点的相似性为$S_T$和$S_F$:</p>

\[S_T=Z_{CTnor}\cdot Z_{CTnor}^T\]

\[S_F=Z_{CFnor}\cdot Z_{CFnor}^T\tag{11}\]

<p>一致性意味着两个相似性矩阵应该相似，这便有了以下约束：</p>

\[\mathscr{L}_c=||S_T-S_F||^2_F\tag{12}\]

<h5 id="342-disparity-constraint">3.4.2 Disparity Constraint</h5>
<p>因为嵌入$Z_T$和$Z_{CF}$是从图$G_t=(A_t,X_t)$中学习的，为了确保它们捕捉到不同的
信息，作者应用了Hilbert-Schmidt独立性标准(HSIC)用来区分这两者的独立性。</p>

\[HSIC(Z_T,Z_{CT})=(n-1)^{-2}tr(RK_TRK_{CT})\tag{13}\]

<p>其中，$K_T$和$K_{CT}$是带有$k_{T,ij}=k_T(z_T^i,Z_T^j)$和$K_{CT,ij}=k_{CT}(z_{CT}^i,z_{CT}^j)$。
,$R=I-/frac{1}{n}ee^T$，其中$I$是单位矩阵，$e$是一个all-one column vector。</p>

<p>对于$HSIC(Z_F,Z_{CF})=(n-1)^{-2}tr(RK_FRK_{CF})\tag{14}$</p>

<p>disparity constraint可以计算为</p>

\[\mathscr{L}_d=HSIC(Z_T,Z_{CT})+HSIC(Z_F,Z_{CF})\tag(15)\]

<h5 id="343-optimization-objective">3.4.3 Optimization Objective.</h5>
<p>在$E_q$中使用输出嵌入$Z$，对于半监督多分类任务使用一个线性转换器和softmax函数。
标记n个结点类预测为$\hat{Y}=[\hat{y}<em>{ic}]\in\mathbb{R}^{n\times C}$，
其中，$\hat{y}</em>{ic}$是结点$i$对于$c$个类的概率。</p>

\[\hat{Y}=softmax(W\cdot Z+b)\tag{16}\]

<p>其中，$softmax(x)=\frac{exp(x)}{\sum_{c=1}^{C}exp(x_c)}$实际上是所有类的规范器。</p>

<p>假设训练集是$L$,对于每个$l\in L$的实际标签为$Y_l$,预测标签为$\hat{Y}_l$。
然后，将所有训练节点上的节点分类的交叉熵损失表示为</p>

\[\mathscr{L}_t=-\sum_{l\in L}\sum_{i=1}^CY_lln\hat{Y}_l\tag{17}\]

<p>结合节点分类任务和约束，我们具有以下总体目标函数：</p>

\[\mathscr{L}=\mathscr{L}_t+\gamma\mathscr{L}_c+\beta\mathscr{L}_d\tag{18}\]

<p>其中$\gamma$和$\beta$是一致性和差异性的参数。在标记数据的指导下，我们可以通过反向传播优化建议的模型，并学习节点的嵌入以进行分类。</p>

<h3 id="四experiment"><span id="p5">四、EXPERIMENT</span></h3>
<h4 id="experimental-setup">Experimental Setup</h4>
<h5 id="datasets">Datasets</h5>
<p><img src="../../../../../../img/in-post/2020.09/04/Table 1.jpg" alt="Table 1" />
AM-GCN在六个数据集上进行验证。</p>
<ul>
  <li>
    <p>Citeseer
Citeseer是一个研究论文引文网络，其中节点是出版物，边是引文链接。 节点属性是论文的词袋表示，并且所有节点都分为六个区域。</p>
  </li>
  <li>
    <p>UAI2010
使用了具有3067个节点和56622个边的数据集，该数据集已在图卷积网络中进行了测试。</p>
  </li>
  <li>
    <p>ACM
该网络是从ACM数据集中提取的，其中节点代表论文，并且如果两篇论文的作者相同，则它们之间会有一条边。 所有论文分为3类（数据库，无线通信，数据挖掘）。 这些功能是纸质关键词的词袋表示。</p>
  </li>
  <li>
    <p>BlogCatalog
这是一个由BlogCatalog网站与博客作者及其社交关系组成的社交网络。 节点属性由用户简要表的关键字构成，标签表示作者提供的主题类别，所有节点分为6类。</p>
  </li>
  <li>
    <p>Flickr
Flickr是一个图像和视频托管网站，用户可以在其中通过照片共享进行交互。 它是一个社交网络，其中节点代表用户，边缘代表他们的关系，并且根据用户的兴趣组将所有节点分为9类。</p>
  </li>
  <li>
    <p>CoraFull
这是著名的引文网络Cora数据集的较大版本，其中，节点表示论文，边表示其引用，并根据论文主题对节点进行标记。</p>
  </li>
</ul>

<h5 id="baselines">Baselines</h5>
<ul>
  <li>DeepWalk是一种网络嵌入方法，它使用随机游走来获取上下文信息，并使用skipgram算法来学习网络表示。</li>
  <li>LINE是一种大规模的网络嵌入方法，可分别保留网络的一阶和二阶邻近性。</li>
  <li>Chebyshev是一种基于GCN利用Chebyshev过滤器的方法。</li>
  <li>GCN是一种半监督图卷积网络模型，该模型通过汇总来自邻居的信息来学习节点表示。</li>
  <li>kNN-GCN为了进行比较，而不是传统的拓扑图中，我们使用根据特征矩阵计算的稀疏k最近邻图作为GCN的输入图，并将其表示为kNN-GCN。</li>
  <li>GAT是使用注意力机制来聚合节点特征的图神经网络模型。</li>
  <li>DEMO-Net是用于节点分类的特定于度的图神经网络。</li>
  <li>MixHop是一种基于GCN的方法，它在一个图卷积层中混合了高阶邻居的特征表示。</li>
</ul>

<p><img src="../../../../../../img/in-post/2020.09/04/Table 2.jpg" alt="Table 2" /></p>
:ET
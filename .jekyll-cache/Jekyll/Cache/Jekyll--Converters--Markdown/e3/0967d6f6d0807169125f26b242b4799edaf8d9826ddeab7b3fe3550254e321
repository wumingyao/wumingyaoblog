I"˜Ã<h2 id="ä¸»è¦å†…å®¹">ä¸»è¦å†…å®¹</h2>
<ul>
  <li><a href="#p1">ABSTRACT</a></li>
  <li><a href="#p2">1 INTRODUCTION</a></li>
  <li><a href="#p3">2 BACKGROUND</a></li>
  <li><a href="#p4">3 PROPOSED APPROACH</a></li>
  <li><a href="#p5">4 EMPIRICAL EVALUATION</a></li>
  <li><a href="#p6">5 MINING FSA FOR DETECTING ANDROID MALICIOUS BEHAVIORS</a></li>
  <li><a href="#p7">6 THREATS TO VALIDITY</a></li>
  <li><a href="#p8">7 RELATED WORK</a></li>
  <li><a href="#p9">8 CONCLUSION AND FUTURE WORK</a></li>
</ul>

<h2 id="æ­£æ–‡">æ­£æ–‡</h2>

<h3 id="abstract"><span id="p1">ABSTRACT</span></h3>
<p>Formal specifications are essential but usually unavailable in software systems. Furthermore, writing these specifications is costly and requires skills from developers. Recently, many automated techniques have been proposed to mine specifications in various formats including finite-state automaton (FSA). However, more works in specification mining are needed to further improve the accuracy of the inferred specifications.</p>

<p>In this work, we propose Deep Specification Miner (DSM), a new approach that performs deep learning for mining FSA-based specifications. Our proposed approach uses test case generation to generate a richer set of execution traces for training a Recurrent Neural Network Based Language Model (RNNLM). From these execution traces, we construct a Prefix Tree Acceptor (PTA) and use the learned RNNLM to extract many features. These features are subsequently utilized by clustering algorithms to merge similar automata states in the PTA for constructing a number of FSAs. Then, our approach performs a model selection heuristic to estimate F-measure of FSAs and returns the one with the highest estimated Fmeasure. We execute DSM to mine specifications of 11 target library classes. Our empirical analysis shows that DSM achieves an average F-measure of 71.97%, outperforming the best performing baseline by 28.22%. We also demonstrate the value of DSM in sandboxing Android apps.</p>

<p>æ‘˜è¦:</p>

<p>è¯´æ˜äº†å½¢å¼åŒ–è§„èŒƒçš„é‡è¦æ€§ã€éš¾ç‚¹ä»¥åŠå‰äººçš„å·¥ä½œï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé€’å½’ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆRNNLMï¼‰æŒ–æ˜FSAè§„èŒƒçš„æ–°æ–¹æ³•DSMã€‚ä½¿ç”¨è‘—åçš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ–¹æ³•Randoopæ¥åˆ›å»ºä¸€ç»„æ›´ä¸°å¯Œçš„æ‰§è¡Œè·Ÿè¸ªæ¥è®­ç»ƒRNNLMã€‚ä»ä¸€ç»„é‡‡æ ·çš„æ‰§è¡Œè½¨è¿¹ä¸­æ„é€ ä¸€ä¸ªå‰ç¼€æ ‘æ¥å—å™¨ï¼ˆPTAï¼‰ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ çš„RNNLMæå–PTAçŠ¶æ€çš„è®¸å¤šç‰¹å¾ã€‚ç„¶åï¼Œèšç±»ç®—æ³•åˆ©ç”¨è¿™äº›ç‰¹å¾åˆå¹¶ç›¸ä¼¼çš„è‡ªåŠ¨æœºçŠ¶æ€ï¼Œä½¿ç”¨å„ç§è®¾ç½®æ„é€ è®¸å¤šfsaã€‚ç„¶åé‡‡ç”¨ä¸€ç§å¯å‘å¼çš„æ¨¡å‹é€‰æ‹©æ–¹æ³•æ¥é€‰æ‹©æœ€ç²¾ç¡®çš„FSAï¼Œå¹¶å°†å…¶ä½œä¸ºæœ€ç»ˆæ¨¡å‹è¾“å‡ºã€‚ä½œè€…è¿è¡Œæ‰€æå‡ºçš„æ–¹æ³•æ¥æ¨æ–­11ä¸ªç›®æ ‡åº“ç±»çš„è§„èŒƒã€‚ç»“æœè¡¨æ˜ï¼ŒDSMçš„å¹³å‡F-æµ‹åº¦ä¸º71.97%ï¼Œæ¯”æœ€ä¼˜åŸºçº¿çš„å¹³å‡F-æµ‹åº¦é«˜28.82%ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨DSMæŒ–æ˜çš„FSAæ¥æ£€æµ‹Androidåº”ç”¨ç¨‹åºä¸­æ¶æ„è¡Œä¸ºçš„æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨æ¨æ–­å‡ºçš„FSAä½œä¸ºè¡Œä¸ºæ¨¡å‹æ¥æ„é€ ä¸€ä¸ªæ›´å…¨é¢çš„æ²™ç›’ï¼Œè¯¥æ²™ç›’è€ƒè™‘æ•æ„ŸAPIæ–¹æ³•çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ä½¿é»„èœ‚çš„çœŸé˜³æ€§ç‡æé«˜15.69%ï¼Œè€Œå‡é˜³æ€§ç‡ä»…æé«˜4.52%ã€‚</p>

<h3 id="1-introduction"><span id="p2">1 INTRODUCTION</span></h3>

<p>Due to rapid evolution to meet demands of clients, software applications and libraries are often released without documentedspecifications. Even when formal specifications are available, they may become outdated as software systems quickly evolve  in a short period of time. Finally, writing formal specifications requires necessary skill and motivation from developers, as this is a costly and time consuming process . Furthermore, the lack of specifications negatively impacts the maintainability and reliability of systems. With no documented specifications, developers may find it difficult to comprehend a piece of code and software is more likely to have bugs due to mistaken assumptions. Furthermore, developers cannot utilize state-of-the-art bug finding and testing tools that need formal specifications as an input.</p>

<p>ä½œè€…åˆ†æäº†ç°çŠ¶ï¼Œå½“å‰è½¯ä»¶å¼€å‘éƒ½æ˜¯ä»¥å¿«é€Ÿå¼€å‘ä¸ºä¸»ï¼Œæ‰€ä»¥åœ¨å¼€å‘ä¹‹å‰æ²¡æœ‰æ­£å¼çš„è§„æ ¼è¯´æ˜æ–‡æ¡£ï¼Œå¦å¤–
ç¼–å†™è§„æ ¼æ–‡æ¡£éœ€è¦å¼€å‘äººå‘˜å…·å¤‡å¿…è¦çš„æŠ€èƒ½ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯è€—æ—¶è€—åŠ›çš„ã€‚</p>

<p>Recently, many automated approaches have been proposed to help developers reduce the cost of manually drafting formal specifications. In this work, we focus on the family of specification mining algorithms that infer finite-state automaton (FSA) based specifications from execution traces. Krka et al.  and many other researchers have proposed various FSA-mining approaches that have improved the quality of inferred FSA models as compared to prior solutions. Nevertheless, the quality of mined specifications is not perfect yet, and more works need to be done to make specification mining better. In fact, FSA based specification miners still suffer from many issues. For instance, if methods in input execution traces frequently occur in a particular order or the amount of input traces is too small, FSAs inferred by k-tails and many other algorithms are likely to return FSAs that are not generalized and overfitted to the input execution traces.</p>

<p>ä½œè€…è°ƒç ”äº†è‡ªåŠ¨åŒ–æŒ–æ˜è§„èŒƒçš„ç›¸å…³å·¥ä½œï¼Œæ€»ç»“äº†å‰äººå·¥ä½œçš„ä¼˜ç¼ºç‚¹ã€‚ç›®å‰çš„è§„èŒƒæŒ–æ˜ç®—æ³•ä»æ‰§è¡Œè½¨è¿¹ä¸­æ¨æ–­å‡ºåŸºäºæœ‰é™çŠ¶æ€è‡ªåŠ¨æœºï¼ˆFSAï¼‰çš„è§„èŒƒã€‚
ä¸å…ˆå‰çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•æé«˜äº†æ¨æ–­çš„FSAæ¨¡å‹çš„è´¨é‡ã€‚ç„¶è€Œï¼Œç›®å‰é‡‡å‡ºçš„è§„æ ¼çš„è´¨é‡è¿˜ä¸å®Œå–„ï¼Œéœ€è¦åšæ›´å¤šçš„å·¥ä½œï¼Œä½¿è§„æ ¼æŒ–æ˜æ›´å¥½ã€‚</p>

<p>åŸºäºFSAçš„è§„èŒƒæŒ–æ˜è€…ä»ç„¶é¢ä¸´è®¸å¤šé—®é¢˜ï¼š
ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥æ‰§è¡Œè·Ÿè¸ªä¸­çš„æ–¹æ³•ç»å¸¸ä»¥ç‰¹å®šçš„é¡ºåºå‡ºç°ï¼Œæˆ–è€…è¾“å…¥è·Ÿè¸ªçš„æ•°é‡å¤ªå°‘ï¼Œåˆ™ç”±k-tailså’Œè®¸å¤šå…¶ä»–ç®—æ³•æ¨æ–­çš„fsaå¯èƒ½ä¼šè¿”å›æœªè¢«æ³›åŒ–å’Œè¿‡åº¦æ‹Ÿåˆåˆ°è¾“å…¥æ‰§è¡Œè·Ÿè¸ªçš„fsaã€‚</p>

<p>To mine more accurate FSA models, we propose a new specification mining algorithm that performs deep learning on execution traces. We name our approach DSM which stands for Deep Specification Miner. Our approach takes as input a target library class C and employs an automated test case generation tool to generate thousands of test cases. The goal of this test case generation process is to capture a richer set of valid sequences of invoked methods of C. Next, we perform deep learning on execution traces of generated test cases to train a Recurrent Neural Network Language Model (RNNLM). After this step, we construct a Prefix Tree Acceptor (PTA) from the execution traces and leverage the learned language model to extract a number of interesting features from PTAâ€™s nodes. These features are then input to clustering algorithms for merging similar states (i.e., PTAâ€™s nodes). The output of an application of a clustering algorithm is a simpler and more generalized FSA that reflects the training execution traces. Finally, our approach predicts the accuracy of constructed FSAs (generated by different clustering algorithms considering different settings) and outputs the one with highest predicted value of F-measure.</p>

<p>ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è§„æ ¼æ–‡æ¡£æŒ–æ˜ç®—æ³•ç”¨äºæŒ–æ˜æ›´ä¸ºç²¾ç¡®çš„FSAæ¨¡å‹ï¼Œè¿™ä¸ªç®—æ³•ä¸»è¦æ˜¯å¯¹æ‰§è¡Œè½¨è¿¹è¿›è¡Œæ·±åº¦å­¦ä¹ ã€‚
ä½œè€…åˆ©ç”¨è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„å·¥å…·ç”Ÿæˆæ•°åƒä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶å°†è¿™äº›æµ‹è¯•ç”¨ä¾‹åšä¸ºè®­ç»ƒæ ·æœ¬å¯¹æ‰§è¡Œè½¨è¿¹è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œ
ä»¥è®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆRNNLMï¼‰ã€‚éšåï¼Œä½œè€…ä»æ‰§è¡Œè½¨è¿¹ä¸­æ„é€ ä¸€ä¸ªå‰ç¼€æ ‘æ¥å—å™¨ï¼ˆPTAï¼‰ï¼Œ
å¹¶åˆ©ç”¨æ‰€å­¦ä¹ çš„è¯­è¨€æ¨¡å‹ä»PTAçš„èŠ‚ç‚¹ä¸­æå–ç‰¹å¾ï¼Œæœ€åï¼Œé€šè¿‡èšç±»ç”ŸæˆFSAã€‚</p>

<p>We evaluate our proposed approach for 11 target library classes which were used before to evaluate many prior work . For each of the input class, we first run Randoop to generate thousands of test cases. Then, we use execution traces generated by running these test cases to infer FSAs. Our experiments show that DSM achieves an average F-measure of 71.97%. Compared to other existing specification mining algorithms, our approach outperforms all baselines that construct FSAs from execution traces  by at least 28.22%. Some of the baselines first use Daikon to learn invariants that are then used to infer a better FSA. Our approach does not use Daikon invariants in the inference of FSAs. Excluding baselines that use Daikon invariants, our approach can outperform the remaining best performing miner by 33.24% in terms of average F-measure.</p>

<p>ä½œè€…é’ˆå¯¹11ä¸ªç›®æ ‡åº“ç±»è¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå®éªŒè¡¨æ˜ï¼ŒDSMçš„å¹³å‡F-æµ‹åº¦ä¸º71.97%ã€‚ä¸å…¶ä»–ç°æœ‰çš„è§„èŒƒæŒ–æ˜ç®—æ³•ç›¸æ¯”ï¼Œä½œè€…çš„æ–¹æ³•çš„æ€§èƒ½æ¯”æ‰€æœ‰ä»æ‰§è¡Œè·Ÿè¸ªæ„é€ fsaçš„åŸºçº¿è‡³å°‘é«˜å‡º28.22%ã€‚</p>

<p>Additionally, we assess the applicability of FSAs mined by DSM in detecting malicious behaviors in Android apps. We propose a technique that leverages a FSA output by DSM mining algorithm as a behavior model to construct an Android sandbox. Our technique outputs a comprehensive sandbox that considers execution context of sensitive API methods to better protect app users. Our comparative evaluation finds that our technique can increase the True Positive Rate of Boxmate, a state-of-the-art sandbox mining approach, by 15.69%, while only increasing False Positive Rate by 4.52%. Replacing DSM with the best performing applicable baseline results in a sandbox that can achieve a similar True Positive Rate (as DSM) but substantially worse False Positive Rate (i.e., False Positive Rate increases by close to 10%). The results indicate it is promising to employ FSAs mined by DSM to create more effective Android sandboxes.</p>

<p>æ­¤å¤–ï¼Œä½œè€…è¿˜è¯„ä¼°äº†DSMæŒ–æ˜çš„fsaåœ¨Androidåº”ç”¨ç¨‹åºæ¶æ„è¡Œä¸ºæ£€æµ‹ä¸­çš„é€‚ç”¨æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨DSMæŒ–æ˜çš„FSAæ¥åˆ›å»ºæ›´æœ‰æ•ˆçš„Androidæ²™ç›’æ˜¯æœ‰å¸Œæœ›çš„ã€‚</p>

<p>The contributions of our work are highlighted below:   <br />
(1)	We propose DSM (Deep Specification Miner), a new specification mining algorithm that utilizes test case generation, deep learning, clustering, and model selection strategy to infer FSA based specifications. To the best of our knowledge, we are the first to use deep learning for mining specifications.    <br />
(2)	We evaluate the effectiveness of DSM on 11 different target library classes. Our results show that our approach outperforms the best baseline by a substantial margin in terms of average F-measure.                <br />
(3)	We propose a technique that employs a FSA inferred by DSM to construct a more comprehensive sandbox that considers execution context of sensitive API methods. Our evaluation shows that our proposed technique can outperform several baselines by a substantial margin in terms of either True Positive Rate or False Positive Rate.</p>

<p>æœ¬ç ”ç©¶çš„è´¡çŒ®ï¼š      <br />
(1)æå‡ºäº†ä¸€ç§æ–°çš„è§„èŒƒæŒ–æ˜ç®—æ³•DSM    <br />
(2)åœ¨11ä¸ªä¸åŒçš„ç›®æ ‡åº“ç±»ä¸Šè¯„ä¼°äº†DSMçš„æœ‰æ•ˆæ€§ã€‚    <br />
(3)æå‡ºä¸€ç§æŠ€æœ¯ï¼Œåˆ©ç”¨DSMæ¨æ–­å‡ºçš„FSAæ„é€ ä¸€ä¸ªæ›´å…¨é¢çš„æ²™ç›’ï¼Œè€ƒè™‘æ•æ„ŸAPIæ–¹æ³•çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚</p>

<h3 id="background"><span id="p2">BACKGROUND</span></h3>

<p>æœ¬ç« ä»‹ç»äº†ä¸¤ç§è¯­è¨€æ¨¡å‹ï¼Œç»Ÿè®¡è¯­è¨€æ¨¡å‹å’ŒåŸºäºè¯­è¨€çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</p>

<h4 id="statistical-language-model">Statistical Language Model</h4>

<p>A statistical language model is an oracle that can foresee how likely a sentence $s=w_1,w_2,\cdots,w_n$ 
to occur in a language. In a nutshell, a statistical language model considers a sequence s to be a list of words $w_1,w_2,\cdots,w_n$ and assigns probability to s by computing joint probability of words: $P(w_1,\cdots,w_n) =\prod_{i=0}^{n-1}P(w_i | w_1,\cdots,w_{iâˆ’1})$. As it is challenging to
compute conditional probability $P(w_i | w_1,\cdots,w_{iâˆ’1})$, each different language model has its own assumption to approximate the calculation. N-grams model, a popular family of language models, approximates in a way that a word wk conditionally depends only on its previous N words (i.e., $w_{kâˆ’N+1},\cdots,w_{kâˆ’1})$. For example, unigram model simply estimates $P(w_i | w_1,\cdots,w_{iâˆ’1})$ as $P(w_i)$, bigram model approximates $P(w_i | w_1,\cdots,w_{iâˆ’1})$ as $P(w_i | w_{iâˆ’1})$, etc. In this work, we utilize the ability of language models to compute
$P(w_i | w_1,\cdots,w_{iâˆ’1})$ for estimating features of automaton states. We consider every method invocation as a word and an execution trace of an object as a sentence (i.e., sequence of method invocations). Given a sequence of previously invoked methods, we use a language model to output the probability of a method to be invoked next.</p>

<p>ä»‹ç»äº†ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼Œç»Ÿè®¡è¯­è¨€æ¨¡å‹æ˜¯ç”¨æ¥é¢„æµ‹æŸä¸ªå¥å­å‡ºç°çš„æ¦‚ç‡çš„æ¨¡å‹ã€‚</p>

<h4 id="recurrent-neural-network-based-language-model">Recurrent Neural Network Based Language Model</h4>
<p>Recently, a family of language models that make use of neural networks is shown to be more effective than n-grams. These models are referred to as neural network based language models (NNLM). If a NNLM has many hidden layers, we refer to the model as a deep neural network language model or deep language model for short. Among these deep language models, Recurrent Neural Network Based Language Model (RNNLM) is well-known with its ability to use internal memories to handle sequences of words with arbitrary lengths. The underlying network architecture of a RNNLM is a Recurrent Neural Network (RNN) that stores information of input word sequences in its hidden layers. Figure 1 demonstrates how a RNN operates given the sequence <START>, STN, NT, HMTF, <END>. In the figure, a RNN is unrolled to become four connected networks, each of which is processing one input method at a time step. Initially, all states in the hidden layer are assigned to zeros. At time $t_k$, a method $m_k$ is represented as an one-hot vector $i_k$ by the input layer. Next, the hidden layer updates its states by using the vector $i_k$ and the states previously computed at time $t_{kâˆ’1}$. Then, the output layer estimates a probability vector ok across all methods for them to appear in the next time step $t_{k+1}$. This process is repeated at subsequent time steps until the last method in the sequence is handled.</END></START></p>

<p>ä»‹ç»äº†åŸºäºè¯­è¨€çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„å•è¯åºåˆ—ã€‚</p>

<h3 id="proposed-approach"><span id="p3">PROPOSED APPROACH</span></h3>

<p>Figure 2 shows the overall framework of our proposed approach. In our framework, there are three major processes: test case generation and traces collection, Recurrent Neural Network Based Language Model (RNNLM) learning, and automata construction. Our approach takes as input a target class and signatures of methods. Then, DSM runs Randoop  to generate a substantial number of test cases for the input target class. Then, we record the execution of these test cases, and retain traces of invocations of methods of the input target class as the training dataset. Next, our approach performs deep learning on the collected traces to infer a RNNLM that is capable of predicting the next likely method to be executed given a sequence of previously called methods. We choose RNNLM over traditional probabilistic language models since past studies show its superiority.</p>

<p>ä½œè€…æ¦‚æ‹¬äº†æ‰€æè®®çš„æ–¹æ³•çš„æ€»ä½“æ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªè¿‡ç¨‹ï¼šæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå’Œè·Ÿè¸ªæ”¶é›†ã€åŸºäºé€’å½’ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹ï¼ˆRNNLMï¼‰å­¦ä¹ å’Œè‡ªåŠ¨æœºæ„å»ºã€‚
è¯¥æ¨¡å‹è¾“å…¥ç›®æ ‡ç±»å’Œæ–¹æ³•ç­¾åï¼Œç„¶åï¼ŒDSMè¿è¡ŒRandoopä¸ºè¾“å…¥ç›®æ ‡ç±»ç”Ÿæˆå¤§é‡æµ‹è¯•ç”¨ä¾‹ã€‚ç„¶åï¼Œè®°å½•è¿™äº›æµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œï¼Œå¹¶å°†å¯¹è¾“å…¥ç›®æ ‡ç±»çš„æ–¹æ³•çš„è°ƒç”¨çš„è·Ÿè¸ªä¿ç•™ä¸ºè®­ç»ƒæ•°æ®é›†ã€‚
æ¥ä¸‹æ¥ï¼Œå¯¹æ”¶é›†åˆ°çš„è·Ÿè¸ªæ‰§è¡Œæ·±å…¥å­¦ä¹ ï¼Œç”¨äºè®­ç»ƒRNNLMï¼Œè¯¥RNNLMèƒ½å¤Ÿé¢„æµ‹ç»™å®šä¸€ç³»åˆ—å…ˆå‰è°ƒç”¨çš„æ–¹æ³•å°†è¦æ‰§è¡Œçš„ä¸‹ä¸€ä¸ªå¯èƒ½çš„æ–¹æ³•ã€‚</p>

<p><img src="../../../../../../img/in-post/2020.06/16/Figure 2.jpg" alt="Figure 2" />
Figure 2: DSMâ€™s Overall Framework</p>

<p>Subsequently, we employ a heuristic to select a subset of traces that best represents the whole training dataset. From these traces, we construct a Prefix Tree Acceptor (PTA); we refer to each PTAâ€™s node as an automaton state. We select the subset of traces in order to optimize the performance when constructing PTA, but still maintaining accuracy of inferred FSAs. Utilizing the inferred RNNLM, we extract a number of features from automaton states, and input the feature values to a number of clustering algorithms (i.e., k-means  and hierarchical clustering  ) considering different settings (e.g., different number of clusters). The output of a clustering algorithm are clusters of similar automaton states. We use these clusters to create a new FSA by merging states that belong to the same cluster. Every application of a clustering algorithm with a particular setting results in a different FSA. We propose a model selection strategy to heuristically select the most accurate model by predicting values of Precision, Recall, and F-measure. Finally, we output the FSA with highest predicted F-measure.</p>

<p>æ ¹æ®è¿™äº›è½¨è¿¹ï¼Œä½œè€…æ„é€ ä¸€ä¸ªå‰ç¼€æ ‘æ¥å—å™¨(PTA)ã€‚
å°†æ¯ä¸ªPTAçš„èŠ‚ç‚¹ç§°ä¸ºä¸€ä¸ªè‡ªåŠ¨æœºçŠ¶æ€ã€‚åˆ©ç”¨æ¨æ–­å‡ºçš„RNNLMï¼Œä»è‡ªåŠ¨æœºçŠ¶æ€ä¸­æå–å‡ºè‹¥å¹²ç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾å€¼è¾“å…¥åˆ°è€ƒè™‘ä¸åŒè®¾ç½®ï¼ˆå¦‚ä¸åŒç°‡æ•°ï¼‰çš„è‹¥å¹²èšç±»ç®—æ³•ï¼ˆå³k-å‡å€¼å’Œå±‚æ¬¡èšç±»ï¼‰ä¸­ã€‚èšç±»ç®—æ³•çš„è¾“å‡ºæ˜¯ç›¸ä¼¼è‡ªåŠ¨æœºçŠ¶æ€çš„èšç±»ã€‚
ä½œè€…ä½¿ç”¨è¿™äº›é›†ç¾¤é€šè¿‡åˆå¹¶å±äºåŒä¸€é›†ç¾¤çš„çŠ¶æ€æ¥åˆ›å»ºæ–°çš„FSAã€‚å…·æœ‰ç‰¹å®šè®¾ç½®çš„èšç±»ç®—æ³•çš„æ¯ä¸ªåº”ç”¨éƒ½ä¼šå¯¼è‡´ä¸åŒçš„FSAã€‚</p>

<h4 id="31-test-case-generation-and-trace-collection">3.1 Test Case Generation and Trace Collection</h4>

<p>This process plays an important role to our approach as it decides the quality of RNNLM inferred by the deep learning process. Previous research works in specification mining collect traces from the execution of a program given unit test cases or inputs manually created by researchers. In this work, we utilize deep learning for mining specification. Deep learning requires a substantially large and rich amount of data. The more training inputs, the more patterns the resultant RNNLM can capture. In general, it is difficult to follow previous works to collect a rich enough set of execution traces for an arbitrary target library class. Firstly, it is challenging to look for all projects that use the target library class, especially for classes from new or unreleased libraries. Secondly, existing unit test cases or manually created inputs may not cover many of the possible execution scenarios of methods in a target class.</p>

<p>ä½œè€…åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥æŒ–æ˜è§„èŒƒã€‚æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡ä¸°å¯Œçš„æ•°æ®ã€‚è®­ç»ƒè¾“å…¥è¶Šå¤šï¼Œç”Ÿæˆçš„RNNLMå¯ä»¥æ•è·çš„æ¨¡å¼è¶Šå¤šã€‚</p>

<p>We address the above issues by following Dallmeier to generate as many test cases as possible for mining specifications, and collect the execution traces of these test cases for subsequent steps. Recently, many test case generation tools have been proposed such as Randoop etc. Among the state-ofthe-art test case generation tools, we choose Randoop because it is widely used and lightweight. Furthermore, Randoop is well maintained and frequently updated with new versions. As future work, we plan to integrate many other test case generation methods into our approach.</p>

<p>Randoopæ˜¯ä¸€æ¬¾æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå·¥å…·ã€‚ä½œè€…é€‰æ‹©Randoopæ˜¯å› ä¸ºå®ƒè¢«å¹¿æ³›ä½¿ç”¨å’Œè½»é‡çº§ã€‚æ­¤å¤–ï¼ŒRandoopç»´æŠ¤è‰¯å¥½ï¼Œå¹¶ç»å¸¸æ›´æ–°æ–°ç‰ˆæœ¬ã€‚</p>

<p>Randoop generates a large number of test cases, which is proportional to the time limit of its execution. In order to improve the coverage of possible sequences of methods under test, we provide class-specific literals aside from default ones to Randoop. For example, for java.net.Socket, we create string and integer literals which are addresses of hosts (e.g., â€œlocalhostâ€, â€œ127.0.0.1â€, etc.) and listening ports (e.g., 8888, etc.). Furthermore, we create driver classes that contain static methods that invoke constructors of the target class to initialize new objects. That helps speed up Randoop to create new objects without spending time to search for appropriate input values for constructors.</p>

<p>ä½œè€…åˆ›å»ºåŒ…å«é™æ€æ–¹æ³•çš„é©±åŠ¨ç¨‹åºç±»ç­‰æ–¹æ³•ï¼Œè°ƒç”¨ç›®æ ‡ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–æ–°å¯¹è±¡ã€‚è¿™æœ‰åŠ©äºåŠ å¿«Randoopåˆ›å»ºæ–°å¯¹è±¡çš„é€Ÿåº¦ï¼Œè€Œæ— éœ€èŠ±æ—¶é—´æœç´¢æ„é€ å‡½æ•°çš„é€‚å½“è¾“å…¥å€¼ã€‚</p>

<h4 id="32-learning-rnnlm-for-specification-mining">3.2 Learning RNNLM for Specification Mining</h4>
<h5 id="321-construction-of-training-method-sequences">3.2.1 Construction of Training Method Sequences</h5>

<p>Our set of collected execution traces is a series of method sequences. Each of these sequences starts and ends with two special symbols: <START> and <END>, respectively. These symbols are used for separating two different sequences. We gather all sequences together to create data for training Recurrent Neural Networks. Furthermore, we limit the maximum frequency of a method sequence MAX_SEQ_FREQ to 10 to prevent imbalanced data issue where a sequence appears much more frequently than the other ones.</END></START></p>

<p>ä½œè€…ä»‹ç»äº†æ„å»ºè®­ç»ƒæ ·æœ¬ï¼Œæ”¶é›†çš„ä¸€ç»„æ‰§è¡Œè·Ÿè¸ªæ˜¯ä¸€ç³»åˆ—æ–¹æ³•åºåˆ—ã€‚æ¯ä¸ªåºåˆ—éƒ½ä»¥ä¸¤ä¸ªç‰¹æ®Šç¬¦å·å¼€å§‹å’Œç»“æŸï¼šåˆ†åˆ«æ˜¯<START>å’Œ<END>ã€‚è¿™äº›ç¬¦å·ç”¨äºåˆ†éš”ä¸¤ä¸ªä¸åŒçš„åºåˆ—ã€‚æˆ‘ä»¬å°†æ‰€æœ‰åºåˆ—é›†åˆåœ¨ä¸€èµ·ï¼Œä¸ºè®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œåˆ›å»ºæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ–¹æ³•åºåˆ—MAX_SEQ_FREQçš„æœ€å¤§é¢‘ç‡é™åˆ¶ä¸º10ï¼Œä»¥é˜²æ­¢åºåˆ—æ¯”å…¶ä»–åºåˆ—æ›´é¢‘ç¹å‡ºç°çš„ä¸å¹³è¡¡æ•°æ®é—®é¢˜ã€‚</END></START></p>

<h5 id="322-model-training">3.2.2 Model Training</h5>
<p>We perform deep learning on the training data to learn a Recurrent Neural Network Based Language Model (RNNLM) for every target library class. By default, we use Long Short-Term Memory (LSTM) network , one of the stateof-the-art RNNs, as the underlying architecture of the RNNLM. Compared to the standard RNN architecture, LSTM is better in learning long-term dependencies. Furthermore, LSTM is scalable for long sequences.</p>

<p>ä½œè€…ä»‹ç»äº†æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ã€‚ä½œè€…å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œä¸ºæ¯ä¸ªç›®æ ‡åº“ç±»å­¦ä¹ ä¸€ä¸ªåŸºäºé€’å½’ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹ï¼ˆRNNLMï¼‰ã€‚</p>

<h4 id="33-automata-construction">3.3 Automata Construction</h4>

<p>In this processing step, our approach takes as input the set of training execution traces and the inferred RNNLM (see Section 3.2). The output of this step is a FSA that best captures the specification of the corresponding target class. The construction of FSA undergoes several substeps: trace sampling, feature extraction, clustering, and model selection.</p>

<p>è‡ªåŠ¨åŒ–æ„å»ºè¿™ä¸ªæ­¥éª¤çš„è¾“å‡ºæ˜¯ä¸€ä¸ªFSAï¼Œå®ƒæœ€å¥½åœ°æ•è·å¯¹åº”ç›®æ ‡ç±»çš„è§„èŒƒã€‚FSAçš„æ„å»ºç»å†äº†è·Ÿè¸ªé‡‡æ ·ã€ç‰¹å¾æå–ã€èšç±»å’Œæ¨¡å‹é€‰æ‹©å‡ ä¸ªæ­¥éª¤ã€‚</p>

<p>At first, we use a heuristic to select a subset of method sequences that represents all training execution traces. The feature extraction and clustering steps use these selected traces, instead of all traces, to reduce computation cost. We construct a Prefix Tree Acceptor (PTA) from the selected traces and extract features for every PTA nodes using the inferred RNNLM. We refer to each PTA node as an automaton state. Figure 3 shows an excerpt of an example PTA constructed from sequences of invocations of methods from java.security.Signature. Our goal is to find similar automaton states and group them into one cluster. In the clustering substep, we run a number of clustering algorithms on PTA nodes with various settings to create many different FSAs. Finally, in the model selection substep, we follow a heuristic to predict the F-measure (see Section 4.2.1) of constructed FSAs and output the one with highest predicted F-measure. The full set of traces is used in this model selection step. In the following paragraphs, we describe details of each substep in this processing step:</p>

<p>ä½œè€…è¯¦ç»†ä»‹ç»äº†è‡ªåŠ¨åŒ–æ„å»ºçš„æ–¹æ³•æ­¥éª¤ã€‚é¦–å…ˆï¼Œä½œè€…ä½¿ç”¨å¯å‘å¼æ–¹æ³•æ¥é€‰æ‹©è¡¨ç¤ºæ‰€æœ‰è®­ç»ƒæ‰§è¡Œè½¨è¿¹çš„æ–¹æ³•åºåˆ—å­é›†ã€‚ç‰¹å¾æå–å’Œèšç±»æ­¥éª¤ä½¿ç”¨è¿™äº›é€‰å®šçš„è·Ÿè¸ªï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚ä»æ‰€é€‰çš„è®°å½•é“ä¸­æ„é€ ä¸€ä¸ªå‰ç¼€æ ‘æ¥æ”¶å™¨ï¼ˆPTAï¼‰ï¼Œå¹¶ä½¿ç”¨æ¨æ–­çš„RNNLMä¸ºæ¯ä¸ªPTAèŠ‚ç‚¹æå–ç‰¹å¾ã€‚ä½œè€…çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ç›¸ä¼¼çš„è‡ªåŠ¨æœºçŠ¶æ€å¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªé›†ç¾¤ã€‚åœ¨èšç±»å­æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬åœ¨PTAèŠ‚ç‚¹ä¸Šè¿è¡Œäº†è®¸å¤šå…·æœ‰ä¸åŒè®¾ç½®çš„èšç±»ç®—æ³•ï¼Œä»¥åˆ›å»ºè®¸å¤šä¸åŒçš„fsaã€‚æœ€åï¼Œåœ¨æ¨¡å‹é€‰æ‹©å­æ­¥éª¤ä¸­ï¼Œä½œè€…é‡‡ç”¨å¯å‘å¼æ–¹æ³•é¢„æµ‹æ„é€ çš„fsaçš„F-æµ‹åº¦ï¼Œå¹¶è¾“å‡ºé¢„æµ‹F-æµ‹åº¦å¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªã€‚</p>

<p>Trace Sampling: Our training data contains a large number of sequences. Thus, it is expensive to use all of them for constructing FSAs. Therefore, the goal of trace sampling is to create a smaller subset that is likely to represent the whole set of all traces reasonably well. We propose a heuristic to find a subset of traces that covers all cooccurrence pairs  of methods in all training traces.</p>

<p>ä½œè€…ä»‹ç»äº†è·Ÿè¸ªé‡‡æ ·çš„è¿‡ç¨‹ã€‚</p>

<p>Feature Extraction: From method sequences of the sampled execution traces, we construct a Prefix Tree Acceptor (PTA). A PTA is a tree-like deterministic finite automaton (DFA) created by putting all the prefixes of sequences as states, and a PTA only accepts the sequences that it is built from. The final states of our constructed PTAs are the ones have incoming edges with <END> labels (see Section 3.2). Figure 3 shows an example of a Prefix Tree Acceptor (PTA). Table 1 shows information of the extracted features. For each state S of a PTA, we are particularly interested in two types of features:</END></p>

<p>(1)	Type I: This type of features captures information of previously invoked methods before the state S is reached. The values of type I features for state S is the occurrences of methods on the path between the starting state (i.e., the root of the PTA) and S. For example, according to Figure 3, the values of Type I features corresponding to node S3 are: 
$F_{\langle START\rangle}=F_{\langle init\rangle}=F_{initVerify}=1$ 
and $F_{update} = F_{verify} = F_{\langle END\rangle} = 0$.</p>

<p>(2)	Type II: This type of features captures the likely methods to be immediately called after a state is reached. Values of these features are computed by the inferred RNNLM in the deep learning step (see Section 3.2). For example, at node S3 in Figure 3, initVerify and <END> have higher probabilities than the other methods to be called afterward. Examples of type II features and their values for node S3 output by a RNNLM are as follows: 
$P_{initVerify} = P_{\langle END\rangle} = 0.4$ and $P_{\langle START\rangle} = P_{\langle init\rangle} = P_{verify} = P_{update} = 0.15$.</END></p>

<p>ä½œè€…ä¸»è¦æå–ä¸¤ç§ç±»å‹çš„ç‰¹å¾ã€‚ ç±»å‹Iï¼šè¿™ç±»ç‰¹æ€§åœ¨åˆ°è¾¾çŠ¶æ€Sä¹‹å‰æ•è·ä»¥å‰è°ƒç”¨çš„æ–¹æ³•çš„ä¿¡æ¯ã€‚çŠ¶æ€Sçš„ç±»å‹Iç‰¹å¾å€¼æ˜¯åœ¨èµ·å§‹çŠ¶æ€ï¼ˆå³PTAçš„æ ¹ï¼‰å’ŒSä¹‹é—´çš„è·¯å¾„ä¸Šå‡ºç°çš„æ–¹æ³•ã€‚
ç±»å‹IIï¼šè¿™ç§ç±»å‹çš„ç‰¹æ€§æ•è·åœ¨åˆ°è¾¾çŠ¶æ€åç«‹å³è°ƒç”¨çš„å¯èƒ½æ–¹æ³•ã€‚</p>

<p>Clustering: We run k-means  and hierarchical clustering algorithms on the PTAâ€™s states with their extracted features. Our goal is to create a simpler and more generalized automaton that captures specifications of a target library class. Since both k-means and hierarchical clustering require the predefined inputC for number of clusters, we try with many values ofC from 2 to MAX_CLUSTER (refer to Section 4.2.2) to search for the best FSA. Overall, the execution of clustering algorithms results in 2 Ã— (MAX_CLUSTER âˆ’ 1) FSAs.</p>

<p>ä½œè€…ä»‹ç»äº†èšç±»è¿‡ç¨‹ã€‚ä½œè€…ä½¿ç”¨k-å‡å€¼å’Œå±‚æ¬¡èšç±»ç®—æ³•å¯¹PTAçš„çŠ¶æ€åŠå…¶æå–çš„ç‰¹å¾è¿›è¡Œèšç±»ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´é€šç”¨çš„è‡ªåŠ¨æœºï¼Œå®ƒæ•è·ç›®æ ‡åº“ç±»çš„è§„èŒƒã€‚</p>

<p>Model Selection: We propose a heuristic to select the best FSA among the ones output by the clustering algorithms. Algorithm 2 describes our strategy to predict precision of an automaton M given the set of all traces Data (see Section 3.1).</p>

<p>ä½œè€…è¿™éƒ¨åˆ†è¯´æ˜äº†æ¨¡å‹çš„é€‰æ‹©ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå¯å‘å¼çš„æ–¹æ³•æ¥ä»èšç±»ç®—æ³•çš„è¾“å‡ºä¸­é€‰æ‹©æœ€ä½³çš„FSAã€‚</p>

<h3 id="-empirical-evaluation"><span id="p4"> EMPIRICAL EVALUATION</span></h3>
<h4 id="41-dataset">4.1 Dataset</h4>
<h5 id="411-target-library-classes">4.1.1 Target Library Classes</h5>
<p>In our experiments, we select 11 target library classes as the benchmark to evaluate the effectiveness of our proposed approach. These library classes were investigated by previous research works in specification mining. Table 2 shows further details of the selected library classes including information of collected execution traces. Among these library classes, 9 out of 11 are from Java Development Kit (JDK); the other two library classes are DataStructure.StackAr (from Daikon project) and NumberFormatStringTokenizer (from Apache Xalan). For every library class, we consider methods that were analyzed by Krka et al.</p>

<p>ä½œè€…é€‰å–äº†11ä¸ªç›®æ ‡åº“ç±»ä½œä¸ºåŸºå‡†æ¥è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™äº›åº“ç±»ä¸­ï¼Œ11ä¸ªåº“ç±»ä¸­æœ‰9ä¸ªæ¥è‡ªJavaå¼€å‘å·¥å…·åŒ…ï¼ˆJDKï¼‰ï¼›å¦å¤–ä¸¤ä¸ªåº“ç±»æ˜¯æ•°æ®ç»“æ„å’Œæ•°å­—æ ¼å¼å­—ç¬¦ä¸²æ ‡è®°å™¨ã€‚</p>

<h5 id="412-ground-truth-models">4.1.2 Ground Truth Models</h5>
<p>We utilize ground truth models created by Krka et al. Among the investigated library classes, we refine ground truth models of five Javaâ€™s Collection based library classes (i.e., ArrayList, LinkedList, HashMap, HashSet, and Hashtable) to capture â€œemptyâ€ and â€œnon-emptyâ€ Table 2: Target Library Classes. â€œ#Mâ€ represents the number of class methods that are analyzed, â€œ#Generated Test Casesâ€ is the number of test cases generated by Randoop, â€œ#Recorded Method Callsâ€ is the number of recorded method calls in the execution traces, â€œNFSTâ€ stands for NumberFormatStringTokenizer.</p>

<p>ä½œè€…åˆ©ç”¨Krkaç­‰äººå»ºç«‹çš„åŸºçº¿æ¨¡å‹ã€‚æ”¹è¿›äº†äº”ä¸ªåŸºäºJavaé›†åˆçš„åº“ç±»çš„æ¨¡å‹ï¼Œä»¥æ•è·â€œç©ºâ€å’Œâ€œéç©ºâ€çš„ç›®æ ‡åº“ç±»ã€‚</p>

<h4 id="42-experimental-settings">4.2 Experimental Settings</h4>
<h5 id="421-evaluation-metrics">4.2.1 Evaluation Metrics</h5>
<p>We follow Lo and Khooâ€™s method  to measure precision and recall for assessing the effectiveness of our proposed approach. Lo and Khooâ€™s method has been widely adopted by many prior specification mining works . Their proposed approach takes as input a ground truth and an inferred FSA. Next, it generates sentences (i.e., traces) from the two FSAs to compute their similarities. Precision of an inferred FSA is the percentage of sentences accepted by its corresponding ground truth model among the ones that are generated by that FSA. Recall of an inferred FSA is the percentage of sentences accepted by itself among the ones that are generated by the corresponding ground truth model. In a nutshell, precision reflects the percentage of sentences produced by an inferred model that are correct, while recall reflects the percentage of correct sentences that an inferred model can produce. We use F-measure, which is the harmonic mean of precision and recall, as a summary metric to evaluate specification mining algorithms. F-measure is defined as follows:
\(F-Measure=2\times\frac{Precision\times Recall}{Precision + Recall}\tag{1}\)</p>

<p>ä½œè€…ä»‹ç»äº†è¯„ä»·æŒ‡æ ‡F-Measure,ç”¨äºè¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
ç²¾ç¡®æ€§åæ˜ äº†æ¨æ–­æ¨¡å‹ç”Ÿæˆçš„æ­£ç¡®å¥å­çš„ç™¾åˆ†æ¯”ï¼Œè€Œå¬å›ç‡åæ˜ äº†æ¨æ–­æ¨¡å‹ç”Ÿæˆçš„æ­£ç¡®å¥å­çš„ç™¾åˆ†æ¯”ã€‚ä½œè€…ä½¿ç”¨ç²¾ç¡®æ€§å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼F-æµ‹åº¦ä½œä¸ºæ€»ç»“åº¦é‡æ¥è¯„ä¼°è§„èŒƒæŒ–æ˜ç®—æ³•ã€‚</p>

<p>To accurately compute precision, recall and F-measure, sentences generated from a FSA must thoroughly cover its states and transitions. To achieve that goal, we set the maximum number of generated sentences to 10,000 with maximal length of 50, and minimum coverage of each transition equals to 20. Similar strategies were adopted in prior works.
ä¸ºäº†å‡†ç¡®è®¡ç®—ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF-æµ‹åº¦ï¼ŒFSAç”Ÿæˆçš„å¥å­å¿…é¡»å®Œå…¨è¦†ç›–å…¶çŠ¶æ€å’Œè½¬æ¢ã€‚</p>

<h5 id="422-experimental-configurations--environments">4.2.2 Experimental Configurations &amp; Environments</h5>
<p>Randoop Configuration. In test case generation step, for each target class, we repeatedly execute Randoop (version 3.1.2) with a time limit of 5 minute with 20 different initial seeds. We set the time limit to 5 minutes to make sure subsequent collected execution traces are not too long as well as not too short. We repeat execution of Randoop 20 times to maximize the coverage of possible sequences of program methods in Randoop generated test cases. Furthermore, we turn off Randoopâ€™s option of generating errorrevealing test cases (i.e., â€“no-error-revealing-tests is set to true) as executions of these test cases are usually interrupted by exceptions or errors, which results in incomplete method sequences for subsequent deep learning process. We find that with this setup the generated traces cover 100% of target API methods; also, on average, 96.97% and 98.18% of edges and states in each target classâ€™ ground-truth model are covered.</p>

<p>ä½œè€…ä»‹ç»äº†Randoopçš„é…ç½®ã€‚åœ¨æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ­¥éª¤ä¸­ï¼Œå¯¹äºæ¯ä¸ªç›®æ ‡ç±»ï¼Œä½œè€…ä½¿ç”¨20ä¸ªä¸åŒçš„åˆå§‹ç§å­ä»¥5åˆ†é’Ÿçš„æ—¶é—´é™åˆ¶é‡å¤æ‰§è¡ŒRandoopã€‚</p>

<h5 id="423-baselines">4.2.3 Baselines</h5>
<p>In the experiments, we compare the effectiveness of DSM with many previous specification mining works . Krka et al. propose a number of algorithms that analyze execution traces to infer FSAs [24]. These algorithms are k-tails, CONTRACTOR++, SEKT, and TEMI. CONTRACTOR++, TEMI, and SEKT infer models leveraging invariants learned using Daikon. On the other hand, k-tails construct models only from ordering of methods in execution traces. Despite the fact that DSM is not processing likely invariants, we include CONTRACTOR++, SEKT, and TEMI as baselines to compare the applicability of deep learning and likely invariant inference in specification mining. For k-tails and SEKT, we choose k âˆˆ {1, 2} following Krka et al. [24] and Le et al. [26]â€™s configurations. In total, we have six different baselines: Traditional 1-tails, Traditional 2-tails,CONTRACTOR++, SEKT 1-tails. SEKT 2-tails, and TEMI.</p>

<p>ä½œè€…å°†è‡ªå·±çš„æ¨¡å‹ä¸å…¶ä»–åŸºçº¿åšäº†å®éªŒè¿›è¡Œå¯¹æ¯”ã€‚</p>

<h3 id="-mining-fsa-for-detecting-android-malicious-behaviors"><span id="p5"> MINING FSA FOR DETECTING ANDROID MALICIOUS BEHAVIORS</span></h3>

<p>Nowadays, Android is the most popular mobile platform with millions of apps and supported devices. As the matter of fact, Android users easily become targets of attackers. Recently, several approaches have been proposed to protect users from potential threats of malware. Among state-of-the-art approaches, Jamrozik et al. propose Boxmate that mines rules to construct Android sandboxes by exploring behaviors of target benign apps. The key idea of Boxmate is it prevents a program to change its behaviour; it can prevent hidden attacks, backdoors, and exploited vulnerabilities from compromising the security of an Android app. Boxmate works on two phases: monitoring and deployment. In the monitoring phase, Boxmate employs a test case generation tool, named Droidmate, to create a rich set of GUI test cases. During the execution of these test cases, Boxmate records invocations of sensitive API methods (e.g., methods that access cameras, locations, etc.), and use them to create sandbox rules. The rules specify what sensitive API methods are allowed to be invoked during deployment. During deployment, when an app accesses a sensitive API method that is not recorded in the above rules, the sandbox immediately intercepts that operation and raises warning messages to users about the suspicious activity.</p>

<p><img src="../../../../../../img/in-post/2020.06/16/Figure 5.jpg" alt="Figure 5" />
Figure 5: Malware detection framework leveraging behavior models inferred by DSM</p>

<p>å›¾5å±•ç¤ºäº†æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿçš„æ‹Ÿè®®æ¡†æ¶ã€‚è¯¥æ¡†æ¶æœ‰ä¸¤ä¸ªé˜¶æ®µï¼š</p>

<p>Monitoring Phase: This phase accepts as input a benign version of the target Android app. We first leverage GUI test case generation tools (i.e., Monkey [1], GUIRipper [2], PUMA [18], Droidmate [22], and Droidbot [30]) to create a diverse set of test cases. Next, we execute the input app with generated test cases and monitor API methods called. In particular, every time the app invokes a sensitive API method X, we select a sequence ofW previously executed API methods before X and include them to the training traces. Then, we employ DSMâ€™s mining algorithm on the gathered traces to construct a FSA based behavior model BM. The constructed model reflects behaviors of the app when calling sensitive API methods. Subsequently, we employ BM to guide an automaton based sandbox in the deployment phase for malware detection.</p>

<p>è¿™éƒ¨åˆ†ä»‹ç»äº†è¯¥æ¡†æ¶çš„ç¬¬ä¸€ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯ç›‘æ§é˜¶æ®µï¼Œæ­¤é˜¶æ®µæ¥å—ç›®æ ‡Androidåº”ç”¨ç¨‹åºçš„è‰¯æ€§ç‰ˆæœ¬ä½œä¸ºè¾“å…¥ã€‚</p>

<p>Deployment Phase: In this phase, our framework leverages the inferred model BM to build an automaton based sandbox. The sandbox is used to govern and control execution of an Android app. Every time an app invokes a sensitive API X, the sandbox selects the sequence of W previously executed methods before X, and input them to the behavior model BM to classify the invocation of X as malicious or benign. If execution of X is predicted as malicious by model BM, the sandbox informs users by raising warning messages about suspicious activities. Otherwise, the sandbox allows the app to continue its executions without notifying users.</p>

<p>è¿™éƒ¨åˆ†ä»‹ç»äº†è¯¥æ¡†æ¶çš„ç¬¬äºŒä¸ªé˜¶æ®µã€‚ç¬¬äºŒä¸ªé˜¶æ®µæ˜¯éƒ¨ç½²é˜¶æ®µï¼Œåœ¨è¿™ä¸ªé˜¶æ®µåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¡†æ¶åˆ©ç”¨æ¨æ–­å‡ºçš„æ¨¡å‹BMæ¥æ„å»ºä¸€ä¸ªåŸºäºè‡ªåŠ¨æœºçš„æ²™ç›’ã€‚æ²™ç›’ç”¨äºç®¡ç†å’Œæ§åˆ¶Androidåº”ç”¨ç¨‹åºçš„æ‰§è¡Œã€‚</p>

<p>We evaluate our proposed malware detection framework using a dataset of 102 pairs of Android apps that were originally collected by Li et al. Each pair of apps contains one benign app and its corresponding malicious version. The malicious apps are created by injecting malicious code to their corresponding unpacked benign apps [29]. All these apps are real apps that are released to various app markets. Recently, Bao et al. [4] used the above 102 pairs to assess the effectiveness of Boxmateâ€™s mined rules with 5 different test case generation tools (i.e., Monkey [1], GUIRipper [2], PUMA [18], Droidmate [22], and Droidbot [30]). In our evaluation, we utilize execution traces of the 102 Android app pairs collected by Bao et al. [4] . We setW (i.e., number of selected methods before an invoked sensitive API method) to 3, and employ these traces to infer several behavior models by using Boxmate, DSM as well as k-tails (k = 1). We include k-tails (k = 1) since according to Table 4 this is the best baseline mining algorithm that infers FSAs from raw traces of API invocations. We let the comparison between DSM and invariant based miners (i.e., CONTRACTOR++ and TEMI) for future work as Daikon is currently unable to mine invariants for Android apps. Next, we evaluate the effectiveness of inferred behavior models in detecting malware using the following evaluation metrics:</p>

<p><strong>True Positive Rate(TPR)</strong>
\(TPR=\frac{TP}{TP+FN}\tag{2}\)</p>

<p><strong>False Positive Rate(TPR)</strong>
\(FPR=\frac{FP}{FP+TN}\tag{3}\)</p>

<p>ä½œè€…ä½¿ç”¨TPRå’ŒFPRä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡è¯„ä¼°æ¨æ–­çš„è¡Œä¸ºæ¨¡å‹åœ¨æ£€æµ‹æ¶æ„è½¯ä»¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>

<h3 id="threats-to-validity"><span id="p6">THREATS TO VALIDITY</span></h3>
<p>Threats to internal validity. We have carefully checked our implementation, but there are errors that we did not notice. There are also potential threats related to correctness of ground truth models created by Krka et al. [24] that we used. To mitigate this threat, we have compared their models against execution traces collected from Randoop generated test cases as well as textual documentations published by library class writers (e.g., Javadocs). We revised the ground truth models accordingly.</p>

<p>Another threat to validity is related to parameter values of target API methods. We use traces collected by Bao et al. [4] which exclude all parameter values. This is different from Jamrozik et al.â€™s work [21] that excludes most (but not all) parameter values. We decide to exclude all parameter values since all specification mining algorithms considered in this paper (including DSM) produce FSAs that have no constraints on values of parameters. As future work, we plan to extend DSM to generate models that include constraints on parameter values.</p>

<p>ä½œè€…éªŒè¯äº†å¯¹å†…éƒ¨æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”ç›¸åº”åœ°è°ƒæ•´äº†æ¨¡å‹å’Œç›¸åº”çš„å‚æ•°å€¼ã€‚</p>

<p>Threats to External Validity. These threats correspond to the generalizability of our empirical findings. In this work, we have analyzed 11 different library classes. This is larger than the number of target classes used to evaluate many prior studies, e.g., [24, 33, 34]. As future works, we plan to reduce this threat by analyzing more library classes to infer their automaton based specifications.</p>

<p>ä½œè€…éªŒè¯äº†å¯¹å¤–éƒ¨æœ‰æ•ˆæ€§çš„å¨èƒã€‚è¿™äº›å¨èƒä¸ç»éªŒå‘ç°çš„æ™®éæ€§ç›¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†11ä¸ªä¸åŒçš„ç±»åº“ã€‚è¿™æ¯”ç”¨äºè¯„ä¼°è®¸å¤šå…ˆå‰ç ”ç©¶çš„ç›®æ ‡ç±»åˆ«çš„æ•°é‡è¦å¤šã€‚</p>

<p>Threats to Construct Validity. These threats correspond to the usage of evaluation metrics. We have followed Lo and Khooâ€™s approach that uses precision, recall, and F-measure to measure the accuracy of automata output by a specification mining algorithm against ground truth models [32]. Furthermore, Lo and Khooâ€™s approach is well known and has been adopted by many previous research works in specification mining e.g., [5, 6, 8, 13, 24, 27, 31, 33]. Additionally, True Positive Rate and False Positive Rate are wellknown metrics and widely adopted by state-of-the-art approaches in Android malware detection (e.g., [3, 50]).</p>

<p>å¨èƒå»ºæ„æœ‰æ•ˆæ€§ã€‚è¿™äº›å¨èƒå¯¹åº”äºè¯„ä¼°æŒ‡æ ‡çš„ä½¿ç”¨ã€‚ä½œè€…éµå¾ªäº†Loå’ŒKhooçš„æ–¹æ³•ï¼Œä½¿ç”¨ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF-æµ‹åº¦ï¼Œé€šè¿‡é’ˆå¯¹åŸºæœ¬çœŸå€¼æ¨¡å‹çš„è§„èŒƒæŒ–æ˜ç®—æ³•æ¥æµ‹é‡è‡ªåŠ¨æœºè¾“å‡ºçš„å‡†ç¡®æ€§ã€‚</p>

<h3 id="related-work"><span id="p7">RELATED WORK</span></h3>
<p>Mining Specifications. Aside from the state-of-the-art baselines considered in Section 4, there are other related works that mine FSA-based specifications from execution traces. Lo et al. propose SMArTIC that mines a FSA from a set of execution traces [13] using a variant of k-tails algorithm that constructs a probabilistic FSA. Mariani et al. propose k-behavior [36] that constructs an automaton by analyzing one single trace at a time. Walkinshaw and Bogdanov propose an approach that allows users to input temporal properties to support a specification miner to construct a FSA from execution traces [45]. Lo et al. further extend Walkinshaw and Bogdanovâ€™s work to automatically infer temporal properties from execution traces, and use these properties to automatically support model inference process of a specification miner [33]. Synoptic infers three kinds of temporal invariants from execution traces and uses them to generate a concise FSA [6]. SpecForge [26] is a meta-approach that analyzes FSAs inferred by other specification miners and combine them together to create a more accurate FSA. None of the above mentioned approaches employs deep learning.</p>

<p>Deep Learning for Software Engineering Tasks. Recently, deep learning methods are proposed to learn representations of data with multiple levels of abstraction [28]. Researchers have been utilizing deep learning to solve challenging tasks in software engineering [17, 25, 47â€“49]. For example, Gu et al. propose DeepAPI that takes as input queries in natural languages and outputs sequences of API methods that developers should follow [17]. In the nutshell, DeepAPI replies on a RNN based model that can translate a sentence in one language to a new sentence in another language. Different from DeepAPI, DSM takes as input method sequences of an API or library and outputs a finite-state automaton that represents behaviors of that API or library. Prior to our work, deep learning models have not been employed to effectively mine specifications.</p>

<p>Language Models for Software Engineering Tasks. Statistical language models have been utilized for many software engineering tasks. For example, Hindle et al. employ n-gram model on code tokens to demonstrate a high degree of local repetitiveness in source code corpora and leverage it to improve Eclipseâ€™s code completion engine [19]. Several other works have extended Hindle et al. work to build more powerful code completion engines; for example, Raychev et al. leverage n-gram and recurrent neural network language model to recommend likely sequences of method calls to a program with holes [41], while Nguyen et al. leverage Hidden Markov Model to learn API usages from Android app bytecode for recommending APIs [40]. Beyond code completion, Wang et al. use n-gram model to detect bugs by identifying low probability token sequences [46]. Our work uses language model for a different task.</p>

<p>è¿™éƒ¨åˆ†ä½œè€…ä»‹ç»äº†ç›¸å…³å·¥ä½œï¼ŒåŒ…æ‹¬è§„æ ¼æ–‡æ¡£æŒ–æ˜ã€å¯¹äºè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ ä»¥åŠå¯¹äºè½¯ä»¶å·¥ç¨‹çš„è¯­è¨€æ¨¡å‹çš„æ„å»ºç­‰å·¥ä½œã€‚</p>

<h3 id="conclusion-and-future-work"><span id="p8">CONCLUSION AND FUTURE WORK</span></h3>

<p>Formal specifications are helpful for many software processes. In this work, we propose DSM, a new approach that employs Recurrent Neural Network Based Language Model (RNNLM) for mining FSA-based specifications. We apply Randoop, a well-known test cases generation approach, to create a richer set of execution traces for training RNNLM. From a set of sampled execution traces, we construct a Prefix Tree Acceptor (PTA) and extract many features of PTAâ€™s states using the learned RNNLM. These features are then utilized by clustering algorithms to merge similar automata states to construct many FSAs using various settings. Then, we employ a model selection heuristic to select the FSA that is estimated to be the most accurate and output it as the final model. We run our proposed approach to infer specifications of 11 target library classes. Our results show that DSM achieves an average F-measure of 71.97%, outperforming the best performing baseline by 28.82%. Additionally, we propose a technique that employs a FSA mined by DSM to detect malicious behaviors in an Android app. In particular, our technique uses the inferred FSA as a behavior model to construct a more comprehensive sandbox that considers execution context of sensitive API methods. Our evaluation shows that the proposed technique can improve the True Positive Rate of Boxmate by 15.69% while only increasing the False Positive Rate by 4.52%.</p>

<p>As future work, we plan to improve DSMâ€™s effectiveness further by integrating information of likely invariants into our deep learning based framework. We also plan to employ EvoSuite [16] and many other test case generation tools to generate an even more comprehensive set of training traces to improve the effectiveness of DSM. Furthermore, we plan to improve DSM by considering many more clustering algorithms aside from k-means and hierarchical clustering, especially the ones that require no inputs for the number of clusters (e.g., DBSCAN [15], etc.). Finally, we plan to evaluate DSM with more classes and libraries in order to reduce threats to external validity.</p>

<p>ä½œè€…å¯¹æœ¬ç ”ç©¶è¿›è¡Œäº†æ€»ç»“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€’å½’ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆRNNLMï¼‰æŒ–æ˜FSAè§„èŒƒçš„æ–°æ–¹æ³•DSMã€‚ä½¿ç”¨è‘—åçš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ–¹æ³•Randoopæ¥åˆ›å»ºä¸€ç»„æ›´ä¸°å¯Œçš„æ‰§è¡Œè·Ÿè¸ªæ¥è®­ç»ƒRNNLMã€‚ä»ä¸€ç»„é‡‡æ ·çš„æ‰§è¡Œè½¨è¿¹ä¸­æ„é€ ä¸€ä¸ªå‰ç¼€æ ‘æ¥å—å™¨ï¼ˆPTAï¼‰ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ çš„RNNLMæå–PTAçŠ¶æ€çš„è®¸å¤šç‰¹å¾ã€‚ç„¶åï¼Œèšç±»ç®—æ³•åˆ©ç”¨è¿™äº›ç‰¹å¾åˆå¹¶ç›¸ä¼¼çš„è‡ªåŠ¨æœºçŠ¶æ€ï¼Œä½¿ç”¨å„ç§è®¾ç½®æ„é€ è®¸å¤šfsaã€‚ç„¶åé‡‡ç”¨ä¸€ç§å¯å‘å¼çš„æ¨¡å‹é€‰æ‹©æ–¹æ³•æ¥é€‰æ‹©æœ€ç²¾ç¡®çš„FSAï¼Œå¹¶å°†å…¶ä½œä¸ºæœ€ç»ˆæ¨¡å‹è¾“å‡ºã€‚ä½œè€…è¿è¡Œæ‰€æå‡ºçš„æ–¹æ³•æ¥æ¨æ–­11ä¸ªç›®æ ‡åº“ç±»çš„è§„èŒƒã€‚ç»“æœè¡¨æ˜ï¼ŒDSMçš„å¹³å‡F-æµ‹åº¦ä¸º71.97%ï¼Œæ¯”æœ€ä¼˜åŸºçº¿çš„å¹³å‡F-æµ‹åº¦é«˜28.82%ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨DSMæŒ–æ˜çš„FSAæ¥æ£€æµ‹Androidåº”ç”¨ç¨‹åºä¸­æ¶æ„è¡Œä¸ºçš„æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨æ¨æ–­å‡ºçš„FSAä½œä¸ºè¡Œä¸ºæ¨¡å‹æ¥æ„é€ ä¸€ä¸ªæ›´å…¨é¢çš„æ²™ç›’ï¼Œè¯¥æ²™ç›’è€ƒè™‘æ•æ„ŸAPIæ–¹æ³•çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ä½¿é»„èœ‚çš„çœŸé˜³æ€§ç‡æé«˜15.69%ï¼Œè€Œå‡é˜³æ€§ç‡ä»…æé«˜4.52%ã€‚</p>

<p>ä½œä¸ºæœªæ¥çš„å·¥ä½œï¼Œä½œè€…è®¡åˆ’é€šè¿‡å°†å¯èƒ½ä¸å˜é‡çš„ä¿¡æ¯é›†æˆåˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ä¸­ï¼Œè¿›ä¸€æ­¥æé«˜DSMçš„æœ‰æ•ˆæ€§ã€‚ä½œè€…è¿˜è®¡åˆ’ä½¿ç”¨å…¶ä»–æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå·¥å…·æ¥ç”Ÿæˆæ›´å…¨é¢çš„è®­ç»ƒè·Ÿè¸ªé›†ï¼Œä»¥æé«˜DSMçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œé™¤äº†k-å‡å€¼å’Œå±‚æ¬¡èšç±»ä¹‹å¤–ï¼Œè¿˜è®¡åˆ’é€šè¿‡è€ƒè™‘æ›´å¤šçš„èšç±»ç®—æ³•æ¥æ”¹è¿›DSMï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¸éœ€è¦è¾“å…¥èšç±»æ•°é‡çš„ç®—æ³•ã€‚æœ€åä½œè€…è®¡åˆ’ä½¿ç”¨æ›´å¤šçš„ç±»å’Œåº“æ¥è¯„ä¼°DSMï¼Œä»¥å‡å°‘å¯¹å¤–éƒ¨æœ‰æ•ˆæ€§çš„å¨èƒã€‚</p>
:ET
I"t<h2 id="ä¸»è¦å†…å®¹">ä¸»è¦å†…å®¹</h2>
<ul>
  <li><a href="#p1">Additional Details for BERT</a></li>
  <li><a href="#p2">Detailed Experimental Setup</a></li>
  <li><a href="#p3">Related Work</a></li>
  <li><a href="#p4">Bert</a></li>
  <li><a href="#p5">Experiments</a></li>
  <li><a href="#p6">Ablation Studies</a></li>
  <li><a href="#p7"> Conclusion</a></li>
</ul>

<h2 id="å‚è€ƒè®ºæ–‡">å‚è€ƒè®ºæ–‡ï¼š</h2>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf">ã€ŠBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingã€‹</a></p>

<h2 id="æ­£æ–‡">æ­£æ–‡</h2>

<h3 id="a-additional-details-for-bert"><span id="p1">A Additional Details for BERT</span></h3>
<h4 id="a1-illustration-of-the-pre-training-tasks">A.1 Illustration of the Pre-training Tasks</h4>

<p><strong>Masked LM and the Masking Procedure</strong> å‡è®¾æ— æ ‡ç­¾å¥å­æ˜¯my dog is hairy,åœ¨éšæœºmaskçš„è¿‡ç¨‹ä¸­ï¼Œé€‰æ‹©äº†ç¬¬å››ä¸ªè¯hairy,åˆ™masking ç¨‹åºå¯ä»¥æœ‰å¦‚ä¸‹å¤„ç†ï¼š</p>
<ul>
  <li>80%çš„å¯èƒ½æ€§ç”¨[MASK]æ¥ä»£æ›¿è¢«maskçš„è¯ï¼Œå³my dog is hairy $\rightarrow$ my dog is [MASK]</li>
  <li>10%çš„å¯èƒ½ç”¨ä¸€ä¸ªéšæœºçš„å•è¯æ¥æ›¿æ¢ï¼Œå³my dog is hairy $\rightarrow$ my dog is apple</li>
  <li>10%çš„å¯èƒ½ä¿æŒè¯¥è¯ä¸å˜ï¼Œå³my dog is hairy $\rightarrow$ my dog is hairy.è¿™æ ·åšçš„ç›®çš„æ˜¯ä½¿è¡¨è¾¾åå‘äºå®é™…è§‚å¯Ÿåˆ°çš„å•è¯ã€‚<font color="red">(å•¥æ„æ€)</font></li>
</ul>

<p>è¿™ä¸ªè¿‡ç¨‹çš„ä¼˜ç‚¹æ˜¯ï¼ŒTransformerç¼–ç å™¨ä¸çŸ¥é“å®ƒå°†è¢«è¦æ±‚é¢„æµ‹å“ªäº›å•è¯ï¼Œæˆ–è€…å“ªäº›å•è¯å·²è¢«éšæœºå•è¯æ›¿æ¢ï¼Œå› æ­¤å®ƒå¿…é¡»ä¿æŒæ¯ä¸ªè¾“å…¥å•è¯çš„åˆ†å¸ƒå¼ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚
<img src="../../../../../../img/in-post/2020.04/21/Figure 1.jpg" alt="Figure 1" />
Figure 1: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT
uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-toleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly
conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and
OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.</p>

<p><strong>Next Sentence Prediction</strong> NSPä»»åŠ¡æè¿°å¦‚ä¸‹ï¼š
<img src="../../../../../../img/in-post/2020.04/21/Figure 2.jpg" alt="Figure 2" /></p>

<h4 id="a2-pre-training-procedure">A.2 Pre-training Procedure</h4>

<p>ä¸ºäº†ç”Ÿæˆæ¯ä¸ªè®­ç»ƒè¾“å…¥åºåˆ—ï¼Œä½œè€…ä»è¯­æ–™åº“ä¸­æŠ½å–ä¸¤æ®µæ–‡æœ¬ï¼Œå¹¶ç§°ä¹‹ä¸ºâ€œå¥å­â€ï¼Œå°½ç®¡å®ƒä»¬é€šå¸¸æ¯”å•ä¸ªå¥å­é•¿å¾—å¤šï¼ˆä½†ä¹Ÿå¯ä»¥æ›´çŸ­ï¼‰ã€‚ç¬¬ä¸€å¥æ¥å—AåµŒå…¥ï¼Œç¬¬äºŒå¥æ¥å—BåµŒå…¥ã€‚50%çš„å¯èƒ½Aæ˜¯Bçš„ä¸‹ä¸€å¥ï¼Œ50%çš„å¯èƒ½Aæ˜¯éšæœºçš„å¥å­ï¼Œé€‰å‡ºä¸¤ä¸ªå¥å­ç»„åˆåè¯é•¿åº¦$\leq$512çš„æ ·æœ¬ã€‚LM maskingæ˜¯åœ¨wordpieceæ ‡è®°åŒ–ä¹‹ååº”ç”¨çš„ï¼Œæ©è”½ç‡ä¸º15%ã€‚</p>

<p>ä½œè€…è®¾ç½®batch_size=256,steps=1000000,epochs=40ï¼Œæ‰€ç”¨çš„è¯­æ–™åº“è¶…è¿‡33äº¿ä¸ªè¯ã€‚$lr=le-4,\beta_1=0.9,\beta_2=0.999$ï¼ŒL2æƒé‡è¡°å‡0.01ï¼Œå­¦ä¹ ç‡åœ¨å‰10000æ­¥é¢„çƒ­ï¼Œå­¦ä¹ ç‡å‘ˆçº¿æ€§è¡°å‡ã€‚è®­ç»ƒæŸå¤±æ˜¯çœŸå®çš„ä¸‹ä¸€å¥å’Œé¢„æµ‹çš„ä¸‹ä¸€å¥çš„ä¼¼ç„¶å€¼çš„å‡å€¼å’Œã€‚</p>

<h4 id="a3-fine-tuning-procedure">A.3 Fine-tuning Procedure</h4>

<p>å¯¹äºå¾®è°ƒï¼Œé™¤äº†æ‰¹é‡å¤§å°ã€å­¦ä¹ é€Ÿç‡å’Œè®­ç»ƒé˜¶æ®µæ•°ä¹‹å¤–ï¼Œå¤§å¤šæ•°æ¨¡å‹è¶…å‚æ•°ä¸é¢„è®­ç»ƒä¸­çš„ç›¸åŒã€‚è¾å­¦ç‡å§‹ç»ˆä¿æŒåœ¨0.1ã€‚æœ€ä½³è¶…å‚æ•°å€¼æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œä½†åœ¨å®éªŒä¸­å‘ç°ï¼Œä»¥ä¸‹å‚æ•°çš„è®¾å®šé€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼š</p>
<ul>
  <li>Batch sizeï¼š16,32</li>
  <li>Learning rate(Adam)ï¼š5e-5,3e-4,2e-5</li>
  <li>Number of epochsï¼š2,3,4</li>
</ul>

<h3 id="b-detailed-experimental-setup"><span id="p1">B Detailed Experimental Setup</span></h3>
<p><img src="../../../../../../img/in-post/2020.04/21/Figure 3.jpg" alt="Figure 3" />
Figure 3: Illustrations of Fine-tuning BERT on Different Tasks.</p>

<p>ä½¿ç”¨BETTå®ç°å…¶ä»–NLPä»»åŠ¡å¦‚Figure 3æ‰€ç¤ºã€‚</p>
:ET